æœ‰æ²¡æœ‰åŠæ³•é€šè¿‡pythonæœ¬åœ°éƒ¨ç½²aiï¼Ÿï¼ˆæ‰‹åŠ¨ä¸‹è½½å‚æ•°æ–‡ä»¶ï¼Œé€šè¿‡pythonè¿è¡Œï¼Œå°±åƒollamaé‚£æ ·ï¼‰
ChatGPT said:

æœ‰çš„ ğŸ‘ï¼Œè€Œä¸”æ–¹å¼è¿˜ä¸å°‘ï¼Œå®Œå…¨å¯ä»¥åªé  Python + æ‰‹åŠ¨ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶ æ¥æœ¬åœ°è¿è¡Œï¼Œå°±åƒ Ollama é‚£æ ·ã€‚ä¸‹é¢ç»™ä½ æ¢³ç†å‡ æ¡ä¸»æµè·¯çº¿ï¼š

ğŸ”¹1. ç›´æ¥ç”¨ Hugging Face + transformers

é€‚åˆ NLP / Chat ç±»æ¨¡å‹ï¼ˆLLaMAã€Mistralã€Qwen ç­‰ï¼‰ã€‚

pip install transformers accelerate torch


ä¸‹è½½æ¨¡å‹æƒé‡ï¼ˆHugging Face ä¸Šæ‰¾ï¼Œæ¯”å¦‚ Qwen/Qwen2-7B-Instructï¼‰ï¼š

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "./Qwen2-7B-Instruct"  # æœ¬åœ°è·¯å¾„ï¼ˆæå‰ä¸‹å¥½ï¼‰

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

inputs = tokenizer("ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Ÿ", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))


æ¨¡å‹æ–‡ä»¶ä½ å¯ä»¥æ‰‹åŠ¨ä¸‹è½½åˆ°ç¡¬ç›˜ï¼Œæ”¾åœ¨ ./Qwen2-7B-Instruct/ã€‚

æ”¯æŒ GPU / CPU / Apple Siliconã€‚

ç¼ºç‚¹ï¼šå¤§æ¨¡å‹å¯èƒ½æ˜¾å­˜åƒç´§ã€‚

ğŸ”¹2. ä½¿ç”¨ llama.cpp Python ç»‘å®š

é€‚åˆä½é…ç½®ç”µè„‘ï¼Œæœ¬åœ°è¿è¡Œé€Ÿåº¦å¿«ï¼ˆé‡åŒ–æ¨¡å‹å°åˆ° 4GBï¼‰ã€‚

å®‰è£…ï¼š

pip install llama-cpp-python


è¿è¡Œï¼š

from llama_cpp import Llama

llm = Llama(model_path="./qwen2-1.5b-instruct-q4_k_m.gguf")  # æœ¬åœ°ggufæ¨¡å‹

output = llm("ä½ å¥½ï¼Œå¸®æˆ‘å†™ä¸€é¦–å°è¯—", max_tokens=100)
print(output["choices"][0]["text"])


gguf æ ¼å¼æ¨¡å‹å¯åœ¨ Hugging Face ä¸‹è½½ï¼ˆå¾ˆå¤š LLaMA/Qwen/Mistral å·²æœ‰äººé‡åŒ–ï¼‰ã€‚

å ç”¨å°ï¼Œç¬”è®°æœ¬ CPU éƒ½èƒ½è·‘ã€‚

åŸç†ä¸Šè·Ÿ Ollama ç±»ä¼¼ã€‚

ğŸ”¹3. ç”¨ vllm éƒ¨ç½² API

é€‚åˆåšæœåŠ¡ç«¯ï¼ˆåƒ Ollama æä¾›æœ¬åœ° REST APIï¼‰ã€‚

pip install vllm


å¯åŠ¨ï¼š

python -m vllm.entrypoints.api_server --model ./Qwen2-7B-Instruct


ç„¶åä½ å°±æœ‰ä¸€ä¸ªæœ¬åœ°çš„ OpenAI é£æ ¼ APIï¼Œå¯ä»¥ç”¨ requests è°ƒç”¨ï¼š

import requests

resp = requests.post(
    "http://localhost:8000/v1/completions",
    json={"model": "Qwen2-7B-Instruct", "prompt": "å†™ä¸€é¦–ä¸ƒè¨€ç»å¥"}
)
print(resp.json())


ä¼˜ç‚¹ï¼šå’Œ OpenAI API ä¸€æ ·ï¼Œå®¹æ˜“æ¥åˆ°ç°æœ‰ä»£ç é‡Œã€‚

ç¼ºç‚¹ï¼šä¾èµ–æ¯”è¾ƒå¤šï¼Œè¦æ±‚æ˜¾å¡æ˜¾å­˜å¤§ã€‚

ğŸ”¹4. å¯¹æ ‡ Ollamaï¼šç”¨ text-generation-webui

çº¯ Python + WebUIï¼Œå¯æœ¬åœ°åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒ transformersã€llama.cppï¼‰ã€‚

å¯åŠ¨åæœ‰ä¸€ä¸ªç½‘é¡µç•Œé¢ï¼Œè¿˜èƒ½å¼€ APIï¼ŒåŸºæœ¬ç­‰ä»·äºæœ¬åœ° Ollamaã€‚
ğŸ‘‰ ä½ å¯ä»¥ git cloneï¼Œç„¶åæŠŠæ¨¡å‹ä¸‹è½½åˆ° models/ æ–‡ä»¶å¤¹ã€‚

ğŸ“Œ æ€»ç»“

ç”µè„‘é…ç½®é«˜ï¼ˆæœ‰æ˜¾å¡ï¼‰ â†’ æ¨è transformers / vllmã€‚

è½»é‡çº§è¿è¡Œ â†’ æ¨è llama.cpp + llama-cpp-pythonã€‚

æƒ³è¦ Ollama é‚£ç§â€œä¸€é”® API + èŠå¤©â€ä½“éªŒ â†’ ç”¨ text-generation-webuiã€‚

è¦ä¸è¦æˆ‘å¸®ä½ å†™ä¸€ä¸ª æœ€å°å¯è¿è¡Œçš„ Python æœ¬åœ° AI demoï¼ˆllama.cpp ç‰ˆæœ¬ï¼‰ï¼Œåªè¦ä¸‹è½½ä¸ª .gguf æ¨¡å‹å°±èƒ½è·‘ï¼Ÿ

You said:
æŒ‰æ–¹æ³•2æ‰§è¡Œï¼Œä½†æ˜¯æˆ‘ä»¬è¦å¦‚ä½•è®©aièƒ½å¤Ÿä½¿ç”¨ä¹‹å‰çš„èŠå¤©è®°å½•å‘¢ï¼Ÿè¿˜æœ‰ï¼Œè¿™ä¸ªåº“å¯ä»¥æ”¯æŒmacBookçš„appleM
ChatGPT said:

å¥½é—®é¢˜ï¼æ–¹æ³•2ç”¨çš„ llama-cpp-python åº“ï¼Œé»˜è®¤å°±æ˜¯â€œå•è½®â€æ¨ç†ï¼Œè¦æ”¯æŒè¿ç»­å¯¹è¯ï¼Œéœ€è¦ è‡ªå·±ç»´æŠ¤ä¸€ä¸ªå¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆprompt historyï¼‰ã€‚å¦å¤–ï¼Œå®ƒå¯¹ MacBook çš„ Apple Silicon (M1/M2/M3) æ˜¯æ”¯æŒçš„ï¼Œè€Œä¸”è¿˜èƒ½ç›´æ¥è·‘åœ¨ Metal GPU ä¸Šã€‚

1. åœ¨å¯¹è¯ä¸­åŠ å…¥å†å²è®°å½•

å…³é”®æ€è·¯ï¼šæŠŠä¹‹å‰çš„å¯¹è¯æ‹¼æ¥è¿› promptï¼Œç„¶åå†å–‚ç»™æ¨¡å‹ã€‚

ä¾‹å­ï¼š

from llama_cpp import Llama

llm = Llama(
    model_path="./qwen2-1.5b-instruct-q4_k_m.gguf", 
    n_ctx=4096,        # ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆtokenæ•°ï¼‰
    n_threads=4,       # CPUçº¿ç¨‹æ•°ï¼ˆå¯è°ƒï¼‰
    n_gpu_layers=-1    # Apple Silicon ä¸‹å¼€å¯ GPU åŠ é€Ÿï¼ˆMetalï¼‰
)

# èŠå¤©å†å²ï¼Œæ‰‹åŠ¨ç»´æŠ¤
history = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„AIåŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½ï¼"},
    {"role": "assistant", "content": "ä½ å¥½å‘€ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼"},
]

def chat(user_input):
    history.append({"role": "user", "content": user_input})

    # æ‹¼æ¥æˆ prompt
    prompt = ""
    for msg in history:
        if msg["role"] == "system":
            prompt += f"[ç³»ç»Ÿæç¤º] {msg['content']}\n"
        elif msg["role"] == "user":
            prompt += f"ç”¨æˆ·: {msg['content']}\n"
        else:
            prompt += f"åŠ©æ‰‹: {msg['content']}\n"
    prompt += "åŠ©æ‰‹:"  # æç¤ºAIç»§ç»­

    output = llm(prompt, max_tokens=200)
    reply = output["choices"][0]["text"].strip()

    history.append({"role": "assistant", "content": reply})
    return reply

print(chat("ä½ èƒ½å†™ä¸€é¦–è¯—å—ï¼Ÿ"))
print(chat("èƒ½åŠ ä¸Šæœˆäº®å—ï¼Ÿ"))


è¿™æ ·ï¼Œç¬¬äºŒæ¬¡æé—®æ—¶ï¼ŒAI å°±èƒ½â€œçœ‹åˆ°â€ä¹‹å‰çš„ä¸Šä¸‹æ–‡ã€‚

2. Apple Silicon (M1/M2/M3) æ”¯æŒæƒ…å†µ

llama-cpp-python åŸç”Ÿæ”¯æŒ Metal GPUï¼Œå®‰è£…æ—¶åªè¦ç”¨ï¼š

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir


ç„¶ååœ¨ä»£ç é‡Œä¼  n_gpu_layers=-1ï¼Œå³å¯è®©æ‰€æœ‰å±‚ç”¨ Metal GPU åŠ é€Ÿã€‚

è¿™æ ·æ¨ç†é€Ÿåº¦æ¯”çº¯ CPU è¦å¿«å‡ å€ã€‚

3. æç¤º âš ï¸

ä¸Šä¸‹æ–‡æ˜¯æœ‰é™çš„ï¼Œæ¯”å¦‚ä½ è®¾ n_ctx=4096ï¼Œå¯¹è¯å¤ªé•¿ä¼šè¢«æˆªæ–­ï¼ˆéœ€è¦åšâ€œæˆªæ–­å†å²â€ï¼‰ã€‚

å¦‚æœä½ æƒ³ç”¨ çœŸæ­£çš„ Chat æ ¼å¼ï¼ˆåƒ OpenAI API é‚£æ · messages=[...]ï¼‰ï¼Œå¯ä»¥åœ¨å°è£…ä¸€å±‚ï¼ŒæŠŠä¸Šé¢æ‹¼æ¥ prompt çš„é€»è¾‘å†™æˆå‡½æ•°ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ å†™ä¸€ä¸ª å®Œæ•´çš„èŠå¤©å¾ªç¯ demoï¼ˆå‘½ä»¤è¡Œè¾“å…¥ â†’ AI å›å¤ â†’ è‡ªåŠ¨ä¿å­˜ä¸Šä¸‹æ–‡ï¼‰ï¼Œè¿™æ ·ä½ å°±èƒ½åœ¨ç»ˆç«¯é‡Œä¸€ç›´å’Œå®ƒå¯¹è¯äº†ï¼Ÿ